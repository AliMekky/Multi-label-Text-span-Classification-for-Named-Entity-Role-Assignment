{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1. Paths to Annotations and Documents\n",
    "annotation_file = \"/home/ali.mekky/Documents/NLP/Assignment_2/SemEval2024/EN/subtask-1-annotations.txt\"  # Path to the annotation file\n",
    "documents_dir = \"/home/ali.mekky/Documents/NLP/Assignment_2/SemEval2024/EN/raw-documents\"  # Directory where text files are stored\n",
    "\n",
    "# Read annotations as raw text\n",
    "with open(annotation_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    annotation_lines = file.readlines()\n",
    "\n",
    "# Parse annotation lines into a structured format\n",
    "structured_data = []\n",
    "\n",
    "for line in annotation_lines:\n",
    "    # Split the line by tab or another consistent delimiter\n",
    "    parts = line.strip().split(\"\\t\")\n",
    "    file_id = parts[0]  # File identifier (e.g., BG_670.txt)\n",
    "    entity = parts[1]  # Named entity\n",
    "    start = int(parts[2])  # Start offset\n",
    "    end = int(parts[3])  # End offset\n",
    "    roles = parts[4:]  # Dynamic roles\n",
    "\n",
    "    # Load the corresponding document text using the file ID\n",
    "    document_path = os.path.join(documents_dir, file_id)\n",
    "    if os.path.exists(document_path):\n",
    "        with open(document_path, \"r\", encoding=\"utf-8\") as doc_file:\n",
    "            document = doc_file.read()\n",
    "    else:\n",
    "        print(f\"Warning: File {file_id} not found in {documents_dir}\")\n",
    "        document = \"\"\n",
    "\n",
    "    # Append structured data\n",
    "    structured_data.append({\n",
    "        \"File\": file_id,\n",
    "        \"Document\": document,\n",
    "        \"Entity\": entity,\n",
    "        \"Start\": start,\n",
    "        \"End\": end,\n",
    "        \"main_role\": roles[0],\n",
    "        \"fine_grained_roles\": roles[1:]\n",
    "    })\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(structured_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Document</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>main_role</th>\n",
       "      <th>fine_grained_roles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN_UA_103861.txt</td>\n",
       "      <td>The World Needs Peacemaker Trump Again \\n\\n by...</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>791</td>\n",
       "      <td>797</td>\n",
       "      <td>Antagonist</td>\n",
       "      <td>[Spy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EN_UA_103861.txt</td>\n",
       "      <td>The World Needs Peacemaker Trump Again \\n\\n by...</td>\n",
       "      <td>China</td>\n",
       "      <td>1516</td>\n",
       "      <td>1520</td>\n",
       "      <td>Antagonist</td>\n",
       "      <td>[Instigator]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EN_UA_103861.txt</td>\n",
       "      <td>The World Needs Peacemaker Trump Again \\n\\n by...</td>\n",
       "      <td>Hamas</td>\n",
       "      <td>2121</td>\n",
       "      <td>2125</td>\n",
       "      <td>Antagonist</td>\n",
       "      <td>[Terrorist]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EN_UA_103861.txt</td>\n",
       "      <td>The World Needs Peacemaker Trump Again \\n\\n by...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>4909</td>\n",
       "      <td>4920</td>\n",
       "      <td>Protagonist</td>\n",
       "      <td>[Peacemaker, Guardian]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EN_UA_021270.txt</td>\n",
       "      <td>Ukraine's Fate Will Be Decided In Coming Year,...</td>\n",
       "      <td>Yermak</td>\n",
       "      <td>667</td>\n",
       "      <td>672</td>\n",
       "      <td>Antagonist</td>\n",
       "      <td>[Incompetent]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               File                                           Document  \\\n",
       "0  EN_UA_103861.txt  The World Needs Peacemaker Trump Again \\n\\n by...   \n",
       "1  EN_UA_103861.txt  The World Needs Peacemaker Trump Again \\n\\n by...   \n",
       "2  EN_UA_103861.txt  The World Needs Peacemaker Trump Again \\n\\n by...   \n",
       "3  EN_UA_103861.txt  The World Needs Peacemaker Trump Again \\n\\n by...   \n",
       "4  EN_UA_021270.txt  Ukraine's Fate Will Be Decided In Coming Year,...   \n",
       "\n",
       "         Entity  Start   End    main_role      fine_grained_roles  \n",
       "0       Chinese    791   797   Antagonist                   [Spy]  \n",
       "1         China   1516  1520   Antagonist            [Instigator]  \n",
       "2         Hamas   2121  2125   Antagonist             [Terrorist]  \n",
       "3  Donald Trump   4909  4920  Protagonist  [Peacemaker, Guardian]  \n",
       "4        Yermak    667   672   Antagonist           [Incompetent]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "main_role\n",
       "Antagonist     264\n",
       "Protagonist    103\n",
       "Innocent        47\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['main_role'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fine_grained_roles\n",
       "Instigator           49\n",
       "Guardian             40\n",
       "Conspirator          38\n",
       "Incompetent          35\n",
       "Foreign Adversary    35\n",
       "Victim               33\n",
       "Tyrant               29\n",
       "Deceiver             26\n",
       "Saboteur             20\n",
       "Virtuous             19\n",
       "Corrupt              17\n",
       "Peacemaker           15\n",
       "Terrorist            14\n",
       "Underdog             12\n",
       "Martyr               11\n",
       "Rebel                11\n",
       "Bigot                 9\n",
       "Traitor               8\n",
       "Scapegoat             8\n",
       "Exploited             6\n",
       "Spy                   3\n",
       "Forgotten             1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_grained_roles_count = df[\"fine_grained_roles\"].explode().value_counts()\n",
    "\n",
    "# Display results\n",
    "fine_grained_roles_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph containing 'COP27':\n",
      "\n",
      "These climate crazies are apparently willing to allow a continent to freeze this winter over their fears of an over-hyped “problem” peddled by globalists who flew more than 400 carbon-spewing private jets to COP27 in Egypt last week. If they truly believe the hype surrounding climate change, they’re targeting the wrong thing.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def find_paragraph_with_entity(text, entity):\n",
    "    # Split the text into paragraphs\n",
    "    paragraphs = text.split(\"\\n\\n\")  # Assuming paragraphs are separated by double newlines\n",
    "\n",
    "    # Search for the paragraph containing the entity\n",
    "    for paragraph in paragraphs:\n",
    "        if entity in paragraph:\n",
    "            return paragraph  # Return the paragraph if the entity is found\n",
    "\n",
    "    return None  # Return None if the entity is not found in any paragraph\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\"\n",
    "Climate Crazies Fail in Attempt to Vandalize Another Classic Work of Art \n",
    "\n",
    "Another of the world’s most recognized and most valuable pieces of art was the target of climate-change activists. Climate crazies tried — and failed — to glue themselves to Edvard Munch’s 1893 painting “The Scream” in Oslo on Friday.\n",
    "\n",
    "It was yet another example of climate change activists using priceless works of art to protest the use of fossil fuels, which climate zealots believe is leading to global warming. In October, climate activists attacked “Girl with a Pearl Earring” by Johannes Vermeer in The Hague. Climate hysterics were also responsible for an attack on Van Gogh’s “Sunflowers” and John Constable’s “The Hay Wain” in London over the summer.\n",
    "\n",
    "The new climate-related vandalism fad seems to have begun with an attack on Da Vinci’s “Mona Lisa” in which a climate fanatic feigned a disability in order to get close enough to smear a pastry on the painting.\n",
    "\n",
    "In addition, “Peach Trees in Blossom” by Van Gogh; “My Heart’s in the Highlands” by Horatio McCulloch; “Tomson’s Aeolian Harp” by J.M.W. Turner; “The Last Supper” by Giampietrino; “Sistine Madonna” by Raphael; and “Haystacks” by Monet have been targeted by climate hysterics since May.\n",
    "\n",
    "As of yet, none of the artwork has been damaged, due to being protected by glass.\n",
    "\n",
    "Video of Friday’s attack shows two young climate vandals attempting to glue themselves to the artwork. Police apprehended the hooligans, and reported there was some glue residue on the glass that protects the paintings.\n",
    "\n",
    "“I scream for people dying,” one of the activists shouted.\n",
    "\n",
    "“I scream when lawmakers ignore science,” the other shouted.\n",
    "\n",
    "The Norwegian climate activist group Stopp Oljeletinga, which translates to “Stop Oil Exploration,” claimed responsibility for the attack.\n",
    "\n",
    "The group demands that the Norwegian government declare “an immediate halt to all further exploration for oil on the Norwegian continental shelf,” and present “a concrete plan for fair adjustment for today’s oil workers.”\n",
    "\n",
    "A spokesperson for the group claimed that the vandalism was an attempt to “pressure lawmakers into stopping oil exploration.”\n",
    "\n",
    "“We are campaigning against ‘Scream’ because it is perhaps Norway’s most famous painting,” said Astrid Rem, a spokesperson for Stopp Oljeletinga. “There have been lots of similar actions around Europe. They have managed something that no other action has managed: achieve an extremely large amount of coverage and press.”\n",
    "\n",
    "But there’s good press and there’s bad press. These crazy antics are of the bad variety.\n",
    "\n",
    "Norway is one of the world’s top oil exporters and provides oil and natural gas to much of Europe, a continent in the midst of a serious energy crunch brought about partly by the war in Ukraine. Russia, the largest supplier of natural gas to Europe, has severely restricted supplies and has shut down the Nord Stream 2 pipeline, which it claims was sabotaged.\n",
    "\n",
    "Without much-needed Norwegian fossil fuels, Europe could be in for an extremely cold winter.\n",
    "\n",
    "The art world has acknowledged their concern over the new phenomenon:\n",
    "\n",
    "“In recent weeks, there have been several attacks on works of art in international museum collections. The activists responsible for them severely underestimate the fragility of these irreplaceable objects, which must be preserved as part of our world cultural heritage,” read a statement signed by approximately one hundred gallery directors and museums.\n",
    "\n",
    "These climate crazies are apparently willing to allow a continent to freeze this winter over their fears of an over-hyped “problem” peddled by globalists who flew more than 400 carbon-spewing private jets to COP27 in Egypt last week. If they truly believe the hype surrounding climate change, they’re targeting the wrong thing.\n",
    "\"\"\"\n",
    "entity_to_find = \"COP27\"\n",
    "paragraph = find_paragraph_with_entity(text, entity_to_find)\n",
    "\n",
    "if paragraph:\n",
    "    print(f\"Paragraph containing '{entity_to_find}':\\n\\n{paragraph}\")\n",
    "else:\n",
    "    print(f\"The entity '{entity_to_find}' was not found in the text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of paragraphs exceeding 400 words: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the directory containing the text files\n",
    "directory = \"/home/ali.mekky/Documents/NLP/Assignment_2/SemEval2024/EN/raw-documents\"\n",
    "\n",
    "# Initialize a counter for paragraphs exceeding 400 words\n",
    "paragraphs_exceeding_400 = 0\n",
    "\n",
    "# Iterate through each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):  # Only process text files\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Read the text from the file\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "        \n",
    "        # Split the text into paragraphs\n",
    "        paragraphs = text.split(\"\\n\\n\")  # Assuming paragraphs are separated by double newlines\n",
    "        \n",
    "        # Count the number of words in each paragraph and check if it exceeds 400\n",
    "        for paragraph in paragraphs:\n",
    "            word_count = len(paragraph.split())\n",
    "            if word_count > 300:\n",
    "                paragraphs_exceeding_400 += 1\n",
    "\n",
    "# Output the result\n",
    "print(f\"Number of paragraphs exceeding 400 words: {paragraphs_exceeding_400}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph containing the entity (1700, 1716):\n",
      "The Norwegian climate activist group Stopp Oljeletinga, which translates to “Stop Oil Exploration,” claimed responsibility for the attack.\n"
     ]
    }
   ],
   "source": [
    "def get_paragraph_by_entity_offset(text, start_offset, end_offset):\n",
    "    \"\"\"\n",
    "    Given a text and the start and end offsets of an entity, return the paragraph containing the entity.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The full text.\n",
    "        start_offset (int): The starting character offset of the entity.\n",
    "        end_offset (int): The ending character offset of the entity.\n",
    "    \n",
    "    Returns:\n",
    "        str: The paragraph containing the entity, or None if not found.\n",
    "    \"\"\"\n",
    "    # Split the text into paragraphs\n",
    "    paragraphs = text.split(\"\\n\\n\")  # Assuming paragraphs are separated by double newlines\n",
    "    \n",
    "    # Track the start and end indices of each paragraph\n",
    "    current_position = 0\n",
    "    for paragraph in paragraphs:\n",
    "        start = current_position\n",
    "        end = start + len(paragraph)\n",
    "        \n",
    "        # Check if the entity offsets fall within this paragraph\n",
    "        if start <= start_offset < end or start < end_offset <= end:\n",
    "            return paragraph\n",
    "        \n",
    "        # Update the current position (add 2 for double newline delimiter)\n",
    "        current_position = end + 2\n",
    "    \n",
    "    return None  # Return None if no paragraph matches\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\"\n",
    "Climate Crazies Fail in Attempt to Vandalize Another Classic Work of Art \n",
    "\n",
    " Another of the world’s most recognized and most valuable pieces of art was the target of climate-change activists. Climate crazies tried — and failed — to glue themselves to Edvard Munch’s 1893 painting “The Scream” in Oslo on Friday.\n",
    "\n",
    "It was yet another example of climate change activists using priceless works of art to protest the use of fossil fuels, which climate zealots believe is leading to global warming. In October, climate activists attacked “Girl with a Pearl Earring” by Johannes Vermeer in The Hague. Climate hysterics were also responsible for an attack on Van Gogh’s “Sunflowers” and John Constable’s “The Hay Wain” in London over the summer.\n",
    "\n",
    "The new climate-related vandalism fad seems to have begun with an attack on Da Vinci’s “Mona Lisa” in which a climate fanatic feigned a disability in order to get close enough to smear a pastry on the painting.\n",
    "\n",
    "In addition, “Peach Trees in Blossom” by Van Gogh; “My Heart’s in the Highlands” by Horatio McCulloch; “Tomson’s Aeolian Harp” by J.M.W. Turner; “The Last Supper” by Giampietrino; “Sistine Madonna” by Raphael; and “Haystacks” by Monet have been targeted by climate hysterics since May.\n",
    "\n",
    "As of yet, none of the artwork has been damaged, due to being protected by glass.\n",
    "\n",
    "Video of Friday’s attack shows two young climate vandals attempting to glue themselves to the artwork. Police apprehended the hooligans, and reported there was some glue residue on the glass that protects the paintings.\n",
    "\n",
    "“I scream for people dying,” one of the activists shouted.\n",
    "\n",
    "“I scream when lawmakers ignore science,” the other shouted.\n",
    "\n",
    "The Norwegian climate activist group Stopp Oljeletinga, which translates to “Stop Oil Exploration,” claimed responsibility for the attack.\n",
    "\n",
    "The group demands that the Norwegian government declare “an immediate halt to all further exploration for oil on the Norwegian continental shelf,” and present “a concrete plan for fair adjustment for today’s oil workers.”\n",
    "\n",
    "A spokesperson for the group claimed that the vandalism was an attempt to “pressure lawmakers into stopping oil exploration.”\n",
    "\n",
    "“We are campaigning against ‘Scream’ because it is perhaps Norway’s most famous painting,” said Astrid Rem, a spokesperson for Stopp Oljeletinga. “There have been lots of similar actions around Europe. They have managed something that no other action has managed: achieve an extremely large amount of coverage and press.”\n",
    "\n",
    "But there’s good press and there’s bad press. These crazy antics are of the bad variety.\n",
    "\n",
    "Norway is one of the world’s top oil exporters and provides oil and natural gas to much of Europe, a continent in the midst of a serious energy crunch brought about partly by the war in Ukraine. Russia, the largest supplier of natural gas to Europe, has severely restricted supplies and has shut down the Nord Stream 2 pipeline, which it claims was sabotaged.\n",
    "\n",
    "Without much-needed Norwegian fossil fuels, Europe could be in for an extremely cold winter.\n",
    "\n",
    "The art world has acknowledged their concern over the new phenomenon:\n",
    "\n",
    "“In recent weeks, there have been several attacks on works of art in international museum collections. The activists responsible for them severely underestimate the fragility of these irreplaceable objects, which must be preserved as part of our world cultural heritage,” read a statement signed by approximately one hundred gallery directors and museums.\n",
    "\n",
    "These climate crazies are apparently willing to allow a continent to freeze this winter over their fears of an over-hyped “problem” peddled by globalists who flew more than 400 carbon-spewing private jets to COP27 in Egypt last week. If they truly believe the hype surrounding climate change, they’re targeting the wrong thing.\n",
    "\"\"\"\n",
    "\n",
    "start_offset = 1700  # Start index of the entity\n",
    "end_offset = 1716    # End index of the entity\n",
    "\n",
    "paragraph = get_paragraph_by_entity_offset(text, start_offset, end_offset)\n",
    "\n",
    "if paragraph:\n",
    "    print(f\"Paragraph containing the entity ({start_offset}, {end_offset}):\\n{paragraph}\")\n",
    "else:\n",
    "    print(f\"No paragraph found for the entity ({start_offset}, {end_offset}).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               File Original Entity  \\\n",
      "0  EN_UA_103861.txt         Chinese   \n",
      "1  EN_UA_103861.txt           China   \n",
      "2  EN_UA_103861.txt           Hamas   \n",
      "3  EN_UA_103861.txt    Donald Trump   \n",
      "4  EN_UA_021270.txt          Yermak   \n",
      "\n",
      "                                 Processed Paragraph    main_role  \\\n",
      "0  There has been an astounding 6,300% increase i...   Antagonist   \n",
      "1  China is constantly threatening Taiwan and its...   Antagonist   \n",
      "2  On October 7, Israel was invaded by Hamas resu...   Antagonist   \n",
      "3  As the world deals with multiple international...  Protagonist   \n",
      "4  Yermak sought to assure the audience that Zele...   Antagonist   \n",
      "\n",
      "       fine_grained_roles  \n",
      "0                   [Spy]  \n",
      "1            [Instigator]  \n",
      "2             [Terrorist]  \n",
      "3  [Peacemaker, Guardian]  \n",
      "4           [Incompetent]  \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from data.preprocessing import load_annotations_no_tokenization\n",
    "\n",
    "# Add the parent directory (code/) to the Python path\n",
    "annotation_file = \"/home/ali.mekky/Documents/NLP/Assignment_2/SemEval2024/EN/subtask-1-annotations.txt\"\n",
    "documents_dir = \"/home/ali.mekky/Documents/NLP/Assignment_2/SemEval2024/EN/raw-documents\"\n",
    "\n",
    "\n",
    "# Load and preprocess data without tokenization\n",
    "df = load_annotations_no_tokenization(annotation_file, documents_dir)\n",
    "\n",
    "# Display a sample of the processed data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Antagonist', 'Spy']\n",
      "1\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Antagonist', 'Terrorist']\n",
      "1\n",
      "['Protagonist', 'Peacemaker', 'Guardian']\n",
      "0\n",
      "['Antagonist', 'Incompetent']\n",
      "1\n",
      "['Antagonist', 'Incompetent']\n",
      "1\n",
      "['Antagonist', 'Incompetent']\n",
      "1\n",
      "['Antagonist', 'Traitor']\n",
      "1\n",
      "['Antagonist', 'Bigot']\n",
      "1\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Antagonist', 'Foreign Adversary']\n",
      "1\n",
      "['Antagonist', 'Foreign Adversary']\n",
      "1\n",
      "['Antagonist', 'Tyrant']\n",
      "1\n",
      "['Innocent', 'Victim']\n",
      "2\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Antagonist', 'Foreign Adversary']\n",
      "1\n",
      "['Antagonist', 'Foreign Adversary']\n",
      "1\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Protagonist', 'Underdog']\n",
      "0\n",
      "['Antagonist', 'Terrorist']\n",
      "1\n",
      "['Antagonist', 'Corrupt']\n",
      "1\n",
      "['Antagonist', 'Tyrant']\n",
      "1\n",
      "['Antagonist', 'Tyrant']\n",
      "1\n",
      "['Antagonist', 'Tyrant']\n",
      "1\n",
      "['Antagonist', 'Corrupt']\n",
      "1\n",
      "['Antagonist', 'Foreign Adversary']\n",
      "1\n",
      "['Antagonist', 'Corrupt']\n",
      "1\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Antagonist', 'Foreign Adversary']\n",
      "1\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Innocent', 'Victim']\n",
      "2\n",
      "['Antagonist', 'Foreign Adversary']\n",
      "1\n",
      "['Antagonist', 'Foreign Adversary']\n",
      "1\n",
      "['Antagonist', 'Incompetent']\n",
      "1\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Antagonist', 'Foreign Adversary']\n",
      "1\n",
      "['Antagonist', 'Foreign Adversary']\n",
      "1\n",
      "['Antagonist', 'Saboteur']\n",
      "1\n",
      "['Protagonist', 'Underdog']\n",
      "0\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Protagonist', 'Virtuous']\n",
      "0\n",
      "['Antagonist', 'Traitor']\n",
      "1\n",
      "['Protagonist', 'Underdog']\n",
      "0\n",
      "['Antagonist', 'Instigator', 'Foreign Adversary']\n",
      "1\n",
      "['Protagonist', 'Rebel']\n",
      "0\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Antagonist', 'Foreign Adversary']\n",
      "1\n",
      "['Antagonist', 'Foreign Adversary']\n",
      "1\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Antagonist', 'Incompetent']\n",
      "1\n",
      "['Antagonist', 'Incompetent']\n",
      "1\n",
      "['Protagonist', 'Virtuous']\n",
      "0\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Protagonist', 'Virtuous']\n",
      "0\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Protagonist', 'Rebel']\n",
      "0\n",
      "['Antagonist', 'Deceiver']\n",
      "1\n",
      "['Antagonist', 'Deceiver']\n",
      "1\n",
      "['Antagonist', 'Deceiver']\n",
      "1\n",
      "['Antagonist', 'Incompetent']\n",
      "1\n",
      "['Protagonist', 'Rebel']\n",
      "0\n",
      "['Antagonist', 'Tyrant']\n",
      "1\n",
      "['Innocent', 'Exploited']\n",
      "2\n",
      "['Innocent', 'Exploited']\n",
      "2\n",
      "['Innocent', 'Victim']\n",
      "2\n",
      "['Protagonist', 'Virtuous']\n",
      "0\n",
      "['Antagonist', 'Corrupt']\n",
      "1\n",
      "['Antagonist', 'Corrupt']\n",
      "1\n",
      "['Antagonist', 'Deceiver']\n",
      "1\n",
      "['Protagonist', 'Rebel', 'Martyr']\n",
      "0\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Antagonist', 'Saboteur']\n",
      "1\n",
      "['Protagonist', 'Virtuous']\n",
      "0\n",
      "['Protagonist', 'Martyr']\n",
      "0\n",
      "['Innocent', 'Victim']\n",
      "2\n",
      "['Innocent', 'Victim']\n",
      "2\n",
      "['Antagonist', 'Terrorist']\n",
      "1\n",
      "['Antagonist', 'Terrorist']\n",
      "1\n",
      "['Antagonist', 'Incompetent']\n",
      "1\n",
      "['Antagonist', 'Incompetent']\n",
      "1\n",
      "['Protagonist', 'Martyr']\n",
      "0\n",
      "['Antagonist', 'Deceiver']\n",
      "1\n",
      "['Antagonist', 'Terrorist']\n",
      "1\n",
      "['Antagonist', 'Incompetent']\n",
      "1\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Antagonist', 'Conspirator', 'Deceiver']\n",
      "1\n",
      "['Antagonist', 'Conspirator', 'Deceiver']\n",
      "1\n",
      "['Antagonist', 'Spy']\n",
      "1\n",
      "['Antagonist', 'Incompetent']\n",
      "1\n",
      "['Innocent', 'Scapegoat']\n",
      "2\n",
      "['Antagonist', 'Incompetent']\n",
      "1\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Antagonist', 'Corrupt']\n",
      "1\n",
      "['Antagonist', 'Bigot', 'Deceiver']\n",
      "1\n",
      "['Innocent', 'Victim']\n",
      "2\n",
      "['Innocent', 'Victim']\n",
      "2\n",
      "['Antagonist', 'Deceiver']\n",
      "1\n",
      "['Innocent', 'Exploited']\n",
      "2\n",
      "['Antagonist', 'Saboteur', 'Conspirator']\n",
      "1\n",
      "['Antagonist', 'Conspirator']\n",
      "1\n",
      "['Antagonist', 'Saboteur']\n",
      "1\n",
      "['Protagonist', 'Virtuous']\n",
      "0\n",
      "['Antagonist', 'Saboteur', 'Instigator']\n",
      "1\n",
      "['Innocent', 'Scapegoat']\n",
      "2\n",
      "['Antagonist', 'Saboteur', 'Conspirator']\n",
      "1\n",
      "['Antagonist', 'Saboteur']\n",
      "1\n",
      "['Antagonist', 'Saboteur']\n",
      "1\n",
      "['Innocent', 'Exploited', 'Victim']\n",
      "2\n",
      "['Antagonist', 'Deceiver']\n",
      "1\n",
      "['Antagonist', 'Incompetent', 'Deceiver']\n",
      "1\n",
      "['Antagonist', 'Bigot']\n",
      "1\n",
      "['Antagonist', 'Saboteur']\n",
      "1\n",
      "['Innocent', 'Victim']\n",
      "2\n",
      "['Antagonist', 'Saboteur', 'Conspirator']\n",
      "1\n",
      "['Antagonist', 'Conspirator']\n",
      "1\n",
      "['Protagonist', 'Underdog']\n",
      "0\n",
      "['Antagonist', 'Deceiver']\n",
      "1\n",
      "['Antagonist', 'Corrupt']\n",
      "1\n",
      "['Antagonist', 'Deceiver']\n",
      "1\n",
      "['Antagonist', 'Foreign Adversary']\n",
      "1\n",
      "['Antagonist', 'Deceiver']\n",
      "1\n",
      "['Antagonist', 'Foreign Adversary']\n",
      "1\n",
      "['Protagonist', 'Underdog']\n",
      "0\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Innocent', 'Victim']\n",
      "2\n",
      "['Protagonist', 'Rebel']\n",
      "0\n",
      "['Innocent', 'Exploited']\n",
      "2\n",
      "['Antagonist', 'Deceiver']\n",
      "1\n",
      "['Innocent', 'Scapegoat']\n",
      "2\n",
      "['Antagonist', 'Deceiver']\n",
      "1\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Antagonist', 'Incompetent']\n",
      "1\n",
      "['Protagonist', 'Peacemaker']\n",
      "0\n",
      "['Antagonist', 'Tyrant']\n",
      "1\n",
      "['Antagonist', 'Corrupt']\n",
      "1\n",
      "['Antagonist', 'Corrupt']\n",
      "1\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Antagonist', 'Instigator', 'Foreign Adversary']\n",
      "1\n",
      "['Antagonist', 'Incompetent']\n",
      "1\n",
      "['Antagonist', 'Terrorist']\n",
      "1\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Innocent', 'Exploited']\n",
      "2\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Antagonist', 'Deceiver']\n",
      "1\n",
      "['Antagonist', 'Conspirator']\n",
      "1\n",
      "['Antagonist', 'Foreign Adversary']\n",
      "1\n",
      "['Antagonist', 'Foreign Adversary']\n",
      "1\n",
      "['Innocent', 'Victim']\n",
      "2\n",
      "['Antagonist', 'Conspirator']\n",
      "1\n",
      "['Antagonist', 'Conspirator']\n",
      "1\n",
      "['Antagonist', 'Saboteur']\n",
      "1\n",
      "['Innocent', 'Scapegoat']\n",
      "2\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Protagonist', 'Underdog']\n",
      "0\n",
      "['Antagonist', 'Foreign Adversary']\n",
      "1\n",
      "['Protagonist', 'Rebel']\n",
      "0\n",
      "['Innocent', 'Victim']\n",
      "2\n",
      "['Antagonist', 'Incompetent']\n",
      "1\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Protagonist', 'Virtuous']\n",
      "0\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Antagonist', 'Corrupt']\n",
      "1\n",
      "['Antagonist', 'Corrupt']\n",
      "1\n",
      "['Antagonist', 'Tyrant']\n",
      "1\n",
      "['Protagonist', 'Virtuous']\n",
      "0\n",
      "['Protagonist', 'Virtuous']\n",
      "0\n",
      "['Protagonist', 'Virtuous']\n",
      "0\n",
      "['Protagonist', 'Virtuous']\n",
      "0\n",
      "['Antagonist', 'Tyrant']\n",
      "1\n",
      "['Antagonist', 'Tyrant']\n",
      "1\n",
      "['Antagonist', 'Conspirator']\n",
      "1\n",
      "['Antagonist', 'Conspirator']\n",
      "1\n",
      "['Antagonist', 'Incompetent']\n",
      "1\n",
      "['Antagonist', 'Incompetent']\n",
      "1\n",
      "['Protagonist', 'Virtuous']\n",
      "0\n",
      "['Antagonist', 'Traitor']\n",
      "1\n",
      "['Antagonist', 'Incompetent']\n",
      "1\n",
      "['Antagonist', 'Conspirator']\n",
      "1\n",
      "['Antagonist', 'Conspirator']\n",
      "1\n",
      "['Antagonist', 'Conspirator']\n",
      "1\n",
      "['Antagonist', 'Bigot']\n",
      "1\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Antagonist', 'Foreign Adversary']\n",
      "1\n",
      "['Antagonist', 'Deceiver']\n",
      "1\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Antagonist', 'Conspirator']\n",
      "1\n",
      "['Antagonist', 'Conspirator']\n",
      "1\n",
      "['Antagonist', 'Conspirator']\n",
      "1\n",
      "['Antagonist', 'Instigator', 'Tyrant']\n",
      "1\n",
      "['Innocent', 'Victim']\n",
      "2\n",
      "['Antagonist', 'Foreign Adversary']\n",
      "1\n",
      "['Innocent', 'Victim']\n",
      "2\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Protagonist', 'Martyr']\n",
      "0\n",
      "['Innocent', 'Scapegoat']\n",
      "2\n",
      "['Antagonist', 'Tyrant']\n",
      "1\n",
      "['Protagonist', 'Martyr']\n",
      "0\n",
      "['Antagonist', 'Bigot']\n",
      "1\n",
      "['Protagonist', 'Rebel']\n",
      "0\n",
      "['Protagonist', 'Rebel', 'Martyr']\n",
      "0\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Antagonist', 'Tyrant']\n",
      "1\n",
      "['Antagonist', 'Incompetent']\n",
      "1\n",
      "['Innocent', 'Victim']\n",
      "2\n",
      "['Antagonist', 'Traitor']\n",
      "1\n",
      "['Antagonist', 'Foreign Adversary']\n",
      "1\n",
      "['Antagonist', 'Saboteur']\n",
      "1\n",
      "['Protagonist', 'Virtuous']\n",
      "0\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Antagonist', 'Incompetent']\n",
      "1\n",
      "['Antagonist', 'Incompetent']\n",
      "1\n",
      "['Antagonist', 'Incompetent']\n",
      "1\n",
      "['Protagonist', 'Underdog']\n",
      "0\n",
      "['Antagonist', 'Foreign Adversary']\n",
      "1\n",
      "['Innocent', 'Victim']\n",
      "2\n",
      "['Antagonist', 'Tyrant']\n",
      "1\n",
      "['Protagonist', 'Peacemaker']\n",
      "0\n",
      "['Antagonist', 'Instigator', 'Tyrant']\n",
      "1\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Antagonist', 'Incompetent']\n",
      "1\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Antagonist', 'Tyrant']\n",
      "1\n",
      "['Antagonist', 'Conspirator']\n",
      "1\n",
      "['Antagonist', 'Tyrant']\n",
      "1\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Antagonist', 'Deceiver']\n",
      "1\n",
      "['Protagonist', 'Rebel']\n",
      "0\n",
      "['Antagonist', 'Terrorist']\n",
      "1\n",
      "['Antagonist', 'Corrupt']\n",
      "1\n",
      "['Innocent', 'Victim']\n",
      "2\n",
      "['Innocent', 'Victim']\n",
      "2\n",
      "['Antagonist', 'Foreign Adversary']\n",
      "1\n",
      "['Innocent', 'Victim']\n",
      "2\n",
      "['Protagonist', 'Peacemaker']\n",
      "0\n",
      "['Protagonist', 'Peacemaker']\n",
      "0\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Antagonist', 'Traitor']\n",
      "1\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Antagonist', 'Conspirator']\n",
      "1\n",
      "['Protagonist', 'Peacemaker']\n",
      "0\n",
      "['Protagonist', 'Peacemaker']\n",
      "0\n",
      "['Protagonist', 'Peacemaker']\n",
      "0\n",
      "['Protagonist', 'Peacemaker']\n",
      "0\n",
      "['Protagonist', 'Peacemaker']\n",
      "0\n",
      "['Protagonist', 'Peacemaker']\n",
      "0\n",
      "['Antagonist', 'Foreign Adversary']\n",
      "1\n",
      "['Antagonist', 'Conspirator']\n",
      "1\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Antagonist', 'Foreign Adversary']\n",
      "1\n",
      "['Antagonist', 'Conspirator']\n",
      "1\n",
      "['Antagonist', 'Incompetent']\n",
      "1\n",
      "['Antagonist', 'Tyrant']\n",
      "1\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Protagonist', 'Virtuous']\n",
      "0\n",
      "['Protagonist', 'Underdog']\n",
      "0\n",
      "['Antagonist', 'Tyrant']\n",
      "1\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Protagonist', 'Peacemaker']\n",
      "0\n",
      "['Innocent', 'Victim']\n",
      "2\n",
      "['Antagonist', 'Incompetent']\n",
      "1\n",
      "['Antagonist', 'Incompetent']\n",
      "1\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Antagonist', 'Saboteur', 'Conspirator']\n",
      "1\n",
      "['Antagonist', 'Saboteur', 'Conspirator']\n",
      "1\n",
      "['Innocent', 'Scapegoat']\n",
      "2\n",
      "['Antagonist', 'Saboteur']\n",
      "1\n",
      "['Antagonist', 'Saboteur']\n",
      "1\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Innocent', 'Victim']\n",
      "2\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Antagonist', 'Conspirator']\n",
      "1\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Antagonist', 'Traitor']\n",
      "1\n",
      "['Antagonist', 'Incompetent']\n",
      "1\n",
      "['Innocent', 'Victim']\n",
      "2\n",
      "['Innocent', 'Forgotten']\n",
      "2\n",
      "['Innocent', 'Victim']\n",
      "2\n",
      "['Antagonist', 'Foreign Adversary']\n",
      "1\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Innocent', 'Victim']\n",
      "2\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Antagonist', 'Spy']\n",
      "1\n",
      "['Antagonist', 'Foreign Adversary']\n",
      "1\n",
      "['Antagonist', 'Foreign Adversary']\n",
      "1\n",
      "['Antagonist', 'Conspirator']\n",
      "1\n",
      "['Antagonist', 'Foreign Adversary']\n",
      "1\n",
      "['Innocent', 'Victim']\n",
      "2\n",
      "['Antagonist', 'Tyrant']\n",
      "1\n",
      "['Protagonist', 'Martyr']\n",
      "0\n",
      "['Protagonist', 'Martyr']\n",
      "0\n",
      "['Protagonist', 'Martyr']\n",
      "0\n",
      "['Protagonist', 'Martyr']\n",
      "0\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Innocent', 'Victim']\n",
      "2\n",
      "['Innocent', 'Scapegoat']\n",
      "2\n",
      "['Antagonist', 'Deceiver']\n",
      "1\n",
      "['Antagonist', 'Conspirator']\n",
      "1\n",
      "['Antagonist', 'Conspirator']\n",
      "1\n",
      "['Antagonist', 'Tyrant']\n",
      "1\n",
      "['Antagonist', 'Incompetent']\n",
      "1\n",
      "['Protagonist', 'Underdog']\n",
      "0\n",
      "['Antagonist', 'Tyrant']\n",
      "1\n",
      "['Antagonist', 'Bigot']\n",
      "1\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Antagonist', 'Incompetent']\n",
      "1\n",
      "['Antagonist', 'Incompetent']\n",
      "1\n",
      "['Antagonist', 'Instigator', 'Foreign Adversary']\n",
      "1\n",
      "['Innocent', 'Victim']\n",
      "2\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Protagonist', 'Underdog']\n",
      "0\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Antagonist', 'Saboteur']\n",
      "1\n",
      "['Antagonist', 'Foreign Adversary']\n",
      "1\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Antagonist', 'Tyrant']\n",
      "1\n",
      "['Antagonist', 'Tyrant']\n",
      "1\n",
      "['Antagonist', 'Incompetent']\n",
      "1\n",
      "['Protagonist', 'Virtuous']\n",
      "0\n",
      "['Protagonist', 'Peacemaker', 'Virtuous']\n",
      "0\n",
      "['Protagonist', 'Rebel', 'Underdog']\n",
      "0\n",
      "['Antagonist', 'Corrupt']\n",
      "1\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Antagonist', 'Tyrant']\n",
      "1\n",
      "['Antagonist', 'Traitor']\n",
      "1\n",
      "['Antagonist', 'Incompetent']\n",
      "1\n",
      "['Antagonist', 'Corrupt']\n",
      "1\n",
      "['Antagonist', 'Deceiver']\n",
      "1\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Antagonist', 'Foreign Adversary']\n",
      "1\n",
      "['Protagonist', 'Martyr']\n",
      "0\n",
      "['Antagonist', 'Saboteur']\n",
      "1\n",
      "['Antagonist', 'Deceiver']\n",
      "1\n",
      "['Antagonist', 'Bigot']\n",
      "1\n",
      "['Antagonist', 'Conspirator']\n",
      "1\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Innocent', 'Scapegoat']\n",
      "2\n",
      "['Protagonist', 'Virtuous']\n",
      "0\n",
      "['Antagonist', 'Terrorist']\n",
      "1\n",
      "['Innocent', 'Victim']\n",
      "2\n",
      "['Innocent', 'Victim']\n",
      "2\n",
      "['Antagonist', 'Conspirator']\n",
      "1\n",
      "['Antagonist', 'Conspirator']\n",
      "1\n",
      "['Antagonist', 'Conspirator']\n",
      "1\n",
      "['Antagonist', 'Conspirator']\n",
      "1\n",
      "['Antagonist', 'Corrupt', 'Traitor']\n",
      "1\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Protagonist', 'Virtuous']\n",
      "0\n",
      "['Antagonist', 'Tyrant']\n",
      "1\n",
      "['Antagonist', 'Corrupt']\n",
      "1\n",
      "['Antagonist', 'Corrupt']\n",
      "1\n",
      "['Antagonist', 'Tyrant']\n",
      "1\n",
      "['Antagonist', 'Tyrant']\n",
      "1\n",
      "['Innocent', 'Victim']\n",
      "2\n",
      "['Antagonist', 'Incompetent']\n",
      "1\n",
      "['Antagonist', 'Deceiver']\n",
      "1\n",
      "['Antagonist', 'Conspirator']\n",
      "1\n",
      "['Antagonist', 'Deceiver']\n",
      "1\n",
      "['Antagonist', 'Saboteur', 'Conspirator']\n",
      "1\n",
      "['Antagonist', 'Terrorist']\n",
      "1\n",
      "['Antagonist', 'Terrorist']\n",
      "1\n",
      "['Antagonist', 'Terrorist']\n",
      "1\n",
      "['Antagonist', 'Terrorist']\n",
      "1\n",
      "['Antagonist', 'Foreign Adversary']\n",
      "1\n",
      "['Antagonist', 'Foreign Adversary', 'Tyrant']\n",
      "1\n",
      "['Protagonist', 'Peacemaker']\n",
      "0\n",
      "['Antagonist', 'Instigator', 'Tyrant']\n",
      "1\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Antagonist', 'Conspirator']\n",
      "1\n",
      "['Antagonist', 'Conspirator']\n",
      "1\n",
      "['Innocent', 'Victim']\n",
      "2\n",
      "['Antagonist', 'Deceiver']\n",
      "1\n",
      "['Antagonist', 'Bigot']\n",
      "1\n",
      "['Protagonist', 'Rebel']\n",
      "0\n",
      "['Antagonist', 'Conspirator']\n",
      "1\n",
      "['Antagonist', 'Bigot']\n",
      "1\n",
      "['Antagonist', 'Deceiver']\n",
      "1\n",
      "['Protagonist', 'Underdog']\n",
      "0\n",
      "['Innocent', 'Victim']\n",
      "2\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Antagonist', 'Saboteur']\n",
      "1\n",
      "['Protagonist', 'Virtuous']\n",
      "0\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Innocent', 'Victim']\n",
      "2\n",
      "['Protagonist', 'Peacemaker']\n",
      "0\n",
      "['Antagonist', 'Incompetent']\n",
      "1\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Antagonist', 'Instigator']\n",
      "1\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Protagonist', 'Guardian']\n",
      "0\n",
      "['Antagonist', 'Terrorist']\n",
      "1\n",
      "['Antagonist', 'Terrorist']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from data.preprocessing import load_annotations_no_tokenization\n",
    "\n",
    "# Add the parent directory (code/) to the Python path\n",
    "annotation_file = \"/home/ali.mekky/Documents/NLP/Assignment_2/SemEval2024/EN/subtask-1-annotations.txt\"\n",
    "documents_dir = \"/home/ali.mekky/Documents/NLP/Assignment_2/SemEval2024/EN/raw-documents\"\n",
    "\n",
    "\n",
    "annotation_file_test = \"/home/ali.mekky/Documents/NLP/Assignment_2/SemEval2024/EN/subtask-1-entity-mentions.txt\"\n",
    "documents_dir_test = \"/home/ali.mekky/Documents/NLP/Assignment_2/SemEval2024/EN/subtask-1-documents\"\n",
    "\n",
    " \n",
    "# Load and preprocess data without tokenization\n",
    "# df = load_annotations_no_tokenization(annotation_file_test, documents_dir_test, is_test=True)\n",
    "df = load_annotations_no_tokenization(annotation_file, documents_dir, is_test= False)\n",
    "\n",
    "# Display a sample of the processed data\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['File'] == \"EN_CC_100012.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Original Entity</th>\n",
       "      <th>Processed Paragraph</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>main_role</th>\n",
       "      <th>fine_grained_roles</th>\n",
       "      <th>multi_labels</th>\n",
       "      <th>main_role_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [File, Original Entity, Processed Paragraph, Start, End, main_role, fine_grained_roles, multi_labels, main_role_label]\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "none_rows = df[df['Processed Paragraph'].isnull()]\n",
    "none_rows.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Original Entity</th>\n",
       "      <th>Processed Paragraph</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>main_role</th>\n",
       "      <th>fine_grained_roles</th>\n",
       "      <th>multi_labels</th>\n",
       "      <th>main_role_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>EN_CC_100012.txt</td>\n",
       "      <td>World Economic Forum</td>\n",
       "      <td>The &lt;ENTITY_START&gt;World Economic Forum&lt;ENTITY...</td>\n",
       "      <td>87</td>\n",
       "      <td>106</td>\n",
       "      <td>Antagonist</td>\n",
       "      <td>[Conspirator]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>EN_CC_100012.txt</td>\n",
       "      <td>the United Nations</td>\n",
       "      <td>The WEF has enlisted &lt;ENTITY_START&gt;the United ...</td>\n",
       "      <td>691</td>\n",
       "      <td>708</td>\n",
       "      <td>Antagonist</td>\n",
       "      <td>[Conspirator]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>EN_CC_100012.txt</td>\n",
       "      <td>Dr. Carol Baker</td>\n",
       "      <td>&lt;ENTITY_START&gt;Dr. Carol Baker&lt;ENTITY_END&gt; was ...</td>\n",
       "      <td>1775</td>\n",
       "      <td>1789</td>\n",
       "      <td>Antagonist</td>\n",
       "      <td>[Conspirator]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>EN_CC_100012.txt</td>\n",
       "      <td>Bill Gates-</td>\n",
       "      <td>Is anyone surprised that a &lt;ENTITY_START&gt;Bill ...</td>\n",
       "      <td>2228</td>\n",
       "      <td>2238</td>\n",
       "      <td>Antagonist</td>\n",
       "      <td>[Conspirator]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>EN_CC_100012.txt</td>\n",
       "      <td>Biden</td>\n",
       "      <td>&lt;ENTITY_START&gt;Biden&lt;ENTITY_END&gt; is a compromis...</td>\n",
       "      <td>3520</td>\n",
       "      <td>3524</td>\n",
       "      <td>Antagonist</td>\n",
       "      <td>[Corrupt, Traitor]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>EN_CC_100012.txt</td>\n",
       "      <td>Robert F. Kennedy</td>\n",
       "      <td>But it gets even worse, as &lt;ENTITY_START&gt;Rober...</td>\n",
       "      <td>4416</td>\n",
       "      <td>4432</td>\n",
       "      <td>Protagonist</td>\n",
       "      <td>[Guardian]</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 File       Original Entity  \\\n",
       "362  EN_CC_100012.txt  World Economic Forum   \n",
       "363  EN_CC_100012.txt    the United Nations   \n",
       "364  EN_CC_100012.txt       Dr. Carol Baker   \n",
       "365  EN_CC_100012.txt           Bill Gates-   \n",
       "366  EN_CC_100012.txt                 Biden   \n",
       "367  EN_CC_100012.txt     Robert F. Kennedy   \n",
       "\n",
       "                                   Processed Paragraph  Start   End  \\\n",
       "362   The <ENTITY_START>World Economic Forum<ENTITY...     87   106   \n",
       "363  The WEF has enlisted <ENTITY_START>the United ...    691   708   \n",
       "364  <ENTITY_START>Dr. Carol Baker<ENTITY_END> was ...   1775  1789   \n",
       "365  Is anyone surprised that a <ENTITY_START>Bill ...   2228  2238   \n",
       "366  <ENTITY_START>Biden<ENTITY_END> is a compromis...   3520  3524   \n",
       "367  But it gets even worse, as <ENTITY_START>Rober...   4416  4432   \n",
       "\n",
       "       main_role  fine_grained_roles  \\\n",
       "362   Antagonist       [Conspirator]   \n",
       "363   Antagonist       [Conspirator]   \n",
       "364   Antagonist       [Conspirator]   \n",
       "365   Antagonist       [Conspirator]   \n",
       "366   Antagonist  [Corrupt, Traitor]   \n",
       "367  Protagonist          [Guardian]   \n",
       "\n",
       "                                          multi_labels  main_role_label  \n",
       "362  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...                1  \n",
       "363  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...                1  \n",
       "364  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...                1  \n",
       "365  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...                1  \n",
       "366  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...                1  \n",
       "367  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...                0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,  1708,    24,  1516,   190,  3007,     6,    25, 28696,  5382,\n",
      "          8662,  1215,  4014, 11328, 15698, 25244,   274,     4,  5076, 41552,\n",
      "          5382,  8662,  1215,  9309, 15698,  2002,    11,   188,   469,   412,\n",
      "             4,    20,  4003, 22034, 36951,    16,  3656,     7,   185,    66,\n",
      "          1104,    82,     8, 10628,    97,  4694,     4,     2,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8662"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from torch.optim import AdamW  # Use this instead of transformers.AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from data.dataset import RoleDataset\n",
    "from data.preprocessing import load_and_tokenize_data\n",
    "from models.model import DebertaForMultiLabelClassification, FocalLoss\n",
    "from training.train import train_model\n",
    "from training.evaluate import evaluate_model\n",
    "import torch\n",
    "import numpy as np\n",
    "from data.preprocessing import split_dataset_with_stratification\n",
    "from transformers import get_scheduler\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Paths\n",
    "# save_path = \"preprocessed_data.pt\"\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "text = \"But it gets even worse, as <ENTITY_START>Robert F. Kennedy<ENTITY_END> explained in New York City. The bioweapon is targeted to take out white people and spare other races.\"\n",
    "\n",
    "tokenized_inputs = tokenizer(\n",
    "    text,\n",
    "    max_length=512,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(tokenized_inputs)\n",
    "41552\n",
    "8662"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0, 41552,  5382,  8662,  1215,  4014, 11328, 15698,   387, 12145,\n",
      "         41552,  5382,  8662,  1215,  9309, 15698,    16,    10, 13969, 29771,\n",
      "          8676,    54,    34,    57, 36778,   352,  2183,    66,   730,     7,\n",
      "             5,   720,  1952,    13,  1724,     4,     2,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "INPUT_TEXT = \"<ENTITY_START>Biden<ENTITY_END> is a compromised puppet politician who has been shamelessly selling out America to the globalists for decades.\"\n",
    "tokenized_inputs = tokenizer(\n",
    "    INPUT_TEXT,\n",
    "    max_length=512,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(tokenized_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenized_inputs[\"input_ids\"]\n",
    "entity_start_id = tokenizer.convert_tokens_to_ids(\"<ENTITY_START>\")\n",
    "entity_end_id = tokenizer.convert_tokens_to_ids(\"<ENTITY_END>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(entity_start_id)\n",
    "print(entity_end_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 0 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m entity_start_pos \u001b[39m=\u001b[39m (input_ids \u001b[39m==\u001b[39;49m entity_start_id)\u001b[39m.\u001b[39;49mnonzero(as_tuple\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)[\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mitem() \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m  \u001b[39m# Position after <ENTITY_START>\u001b[39;00m\n\u001b[1;32m      2\u001b[0m entity_end_pos \u001b[39m=\u001b[39m (input_ids \u001b[39m==\u001b[39m entity_end_id)\u001b[39m.\u001b[39mnonzero(as_tuple\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mitem()   \n",
      "\u001b[0;31mRuntimeError\u001b[0m: a Tensor with 0 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "entity_start_pos = (input_ids == entity_start_id).nonzero(as_tuple=True)[1].item() + 1  # Position after <ENTITY_START>\n",
    "entity_end_pos = (input_ids == entity_end_id).nonzero(as_tuple=True)[1].item()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_embeddings = last_hidden_state[0, entity_start_pos:entity_end_pos, :]  # Shape: (entity_length, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50265\n",
      "50266\n",
      "Tokens between <ENTITY_START> and <ENTITY_END>: ['B', 'iden']\n",
      "Embeddings shape: torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Example input text\n",
    "text = \"<ENTITY_START>Biden<ENTITY_END> is a compromised puppet politician who has been shamelessly selling out America to the globalists for decades.\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Add custom tokens for <ENTITY_START> and <ENTITY_END>\n",
    "special_tokens = {\"additional_special_tokens\": [\"<ENTITY_START>\", \"<ENTITY_END>\"]}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(\n",
    "    text,\n",
    "    max_length=512,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Convert special tokens to IDs\n",
    "entity_start_id = tokenizer.convert_tokens_to_ids(\"<ENTITY_START>\")\n",
    "entity_end_id = tokenizer.convert_tokens_to_ids(\"<ENTITY_END>\")\n",
    "\n",
    "print(entity_start_id)\n",
    "print(entity_end_id)\n",
    "\n",
    "# Get input IDs and find positions of <ENTITY_START> and <ENTITY_END>\n",
    "input_ids = inputs[\"input_ids\"].squeeze(0)\n",
    "entity_start_idx = (input_ids == entity_start_id).nonzero(as_tuple=True)[0].item() + 1\n",
    "entity_end_idx = (input_ids == entity_end_id).nonzero(as_tuple=True)[0].item()\n",
    "\n",
    "# Pass the input through the model to get hidden states\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract token embeddings between <ENTITY_START> and <ENTITY_END>\n",
    "hidden_states = outputs.last_hidden_state.squeeze(0)  # Shape: (seq_length, hidden_size)\n",
    "entity_embeddings = hidden_states[entity_start_idx:entity_end_idx]  # Extract embeddings\n",
    "\n",
    "# Extract token IDs and corresponding tokens between the markers\n",
    "tokens_between = input_ids[entity_start_idx:entity_end_idx]\n",
    "tokens_text = tokenizer.convert_ids_to_tokens(tokens_between)\n",
    "\n",
    "# Print results\n",
    "print(f\"Tokens between <ENTITY_START> and <ENTITY_END>: {tokens_text}\")\n",
    "print(f\"Embeddings shape: {entity_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpasg2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
